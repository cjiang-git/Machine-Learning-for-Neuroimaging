{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f97be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84d4119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a57c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b1a96",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c510d",
   "metadata": {},
   "source": [
    "### Make the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098ec94f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = 'data_assignment_1.csv'\n",
    "\n",
    "#Data import\n",
    "df3 = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "023b0410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frontal_Sup</th>\n",
       "      <th>Frontal_Inf</th>\n",
       "      <th>Cingulum_Ant</th>\n",
       "      <th>Cingulum_Post</th>\n",
       "      <th>Parietal_Sup</th>\n",
       "      <th>Parietal_Inf</th>\n",
       "      <th>Occipital_Sup</th>\n",
       "      <th>Occipital_Inf</th>\n",
       "      <th>Temporal_Sup</th>\n",
       "      <th>Temporal_Inf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.541704</td>\n",
       "      <td>0.553985</td>\n",
       "      <td>0.577727</td>\n",
       "      <td>0.502631</td>\n",
       "      <td>0.539654</td>\n",
       "      <td>0.562739</td>\n",
       "      <td>0.584094</td>\n",
       "      <td>0.656325</td>\n",
       "      <td>0.529551</td>\n",
       "      <td>0.521926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.665915</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.525422</td>\n",
       "      <td>0.473192</td>\n",
       "      <td>0.589977</td>\n",
       "      <td>0.656130</td>\n",
       "      <td>0.677115</td>\n",
       "      <td>0.571440</td>\n",
       "      <td>0.430463</td>\n",
       "      <td>0.499023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.385500</td>\n",
       "      <td>0.535842</td>\n",
       "      <td>0.651637</td>\n",
       "      <td>0.556026</td>\n",
       "      <td>0.451074</td>\n",
       "      <td>0.602841</td>\n",
       "      <td>0.578349</td>\n",
       "      <td>0.626247</td>\n",
       "      <td>0.511696</td>\n",
       "      <td>0.531485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.601210</td>\n",
       "      <td>0.500756</td>\n",
       "      <td>0.614601</td>\n",
       "      <td>0.581634</td>\n",
       "      <td>0.489078</td>\n",
       "      <td>0.615674</td>\n",
       "      <td>0.579836</td>\n",
       "      <td>0.548231</td>\n",
       "      <td>0.499937</td>\n",
       "      <td>0.612477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.529871</td>\n",
       "      <td>0.521045</td>\n",
       "      <td>0.576609</td>\n",
       "      <td>0.557342</td>\n",
       "      <td>0.494459</td>\n",
       "      <td>0.577652</td>\n",
       "      <td>0.668027</td>\n",
       "      <td>0.520755</td>\n",
       "      <td>0.521910</td>\n",
       "      <td>0.503761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.555299</td>\n",
       "      <td>0.597638</td>\n",
       "      <td>0.553309</td>\n",
       "      <td>0.523025</td>\n",
       "      <td>0.525431</td>\n",
       "      <td>0.667517</td>\n",
       "      <td>0.502397</td>\n",
       "      <td>0.601756</td>\n",
       "      <td>0.494372</td>\n",
       "      <td>0.637729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.524961</td>\n",
       "      <td>0.631617</td>\n",
       "      <td>0.382947</td>\n",
       "      <td>0.410978</td>\n",
       "      <td>0.505315</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.605452</td>\n",
       "      <td>0.625818</td>\n",
       "      <td>0.445138</td>\n",
       "      <td>0.534949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.490657</td>\n",
       "      <td>0.652892</td>\n",
       "      <td>0.654747</td>\n",
       "      <td>0.656929</td>\n",
       "      <td>0.521088</td>\n",
       "      <td>0.702478</td>\n",
       "      <td>0.640216</td>\n",
       "      <td>0.537376</td>\n",
       "      <td>0.642808</td>\n",
       "      <td>0.598981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.584763</td>\n",
       "      <td>0.602108</td>\n",
       "      <td>0.465685</td>\n",
       "      <td>0.602878</td>\n",
       "      <td>0.693304</td>\n",
       "      <td>0.707188</td>\n",
       "      <td>0.685981</td>\n",
       "      <td>0.683557</td>\n",
       "      <td>0.697648</td>\n",
       "      <td>0.688851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.498238</td>\n",
       "      <td>0.661749</td>\n",
       "      <td>0.460143</td>\n",
       "      <td>0.661333</td>\n",
       "      <td>0.694017</td>\n",
       "      <td>0.742571</td>\n",
       "      <td>0.596009</td>\n",
       "      <td>0.785511</td>\n",
       "      <td>0.575335</td>\n",
       "      <td>0.539668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Frontal_Sup  Frontal_Inf  Cingulum_Ant  Cingulum_Post  Parietal_Sup  \\\n",
       "0      0.541704     0.553985      0.577727       0.502631      0.539654   \n",
       "1      0.665915     0.477778      0.525422       0.473192      0.589977   \n",
       "2      0.385500     0.535842      0.651637       0.556026      0.451074   \n",
       "3      0.601210     0.500756      0.614601       0.581634      0.489078   \n",
       "4      0.529871     0.521045      0.576609       0.557342      0.494459   \n",
       "..          ...          ...           ...            ...           ...   \n",
       "95     0.555299     0.597638      0.553309       0.523025      0.525431   \n",
       "96     0.524961     0.631617      0.382947       0.410978      0.505315   \n",
       "97     0.490657     0.652892      0.654747       0.656929      0.521088   \n",
       "98     0.584763     0.602108      0.465685       0.602878      0.693304   \n",
       "99     0.498238     0.661749      0.460143       0.661333      0.694017   \n",
       "\n",
       "    Parietal_Inf  Occipital_Sup  Occipital_Inf  Temporal_Sup  Temporal_Inf  \n",
       "0       0.562739       0.584094       0.656325      0.529551      0.521926  \n",
       "1       0.656130       0.677115       0.571440      0.430463      0.499023  \n",
       "2       0.602841       0.578349       0.626247      0.511696      0.531485  \n",
       "3       0.615674       0.579836       0.548231      0.499937      0.612477  \n",
       "4       0.577652       0.668027       0.520755      0.521910      0.503761  \n",
       "..           ...            ...            ...           ...           ...  \n",
       "95      0.667517       0.502397       0.601756      0.494372      0.637729  \n",
       "96      0.611400       0.605452       0.625818      0.445138      0.534949  \n",
       "97      0.702478       0.640216       0.537376      0.642808      0.598981  \n",
       "98      0.707188       0.685981       0.683557      0.697648      0.688851  \n",
       "99      0.742571       0.596009       0.785511      0.575335      0.539668  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.iloc[:, 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38b0908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "     ..\n",
       "95    1\n",
       "96    1\n",
       "97    1\n",
       "98    1\n",
       "99    1\n",
       "Name: diagnosis, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6550ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the data\n",
    "df3_s = df3.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "415c86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k_fold preparation\n",
    "k = 5\n",
    "data_lst = []\n",
    "indices_lst = []\n",
    "\n",
    "for i in range(k):\n",
    "    indices_lst.append(int(len(df3_s)/k)*i)\n",
    "indices_lst.append(len(df3_s))\n",
    "\n",
    "\n",
    "for j in range(k):\n",
    "    #Obtained train_test splitted dataframe\n",
    "    split_test = df3_s[indices_lst[j]:indices_lst[j+1]]\n",
    "    split_train = df3_s.drop(split_test.index)\n",
    "    \n",
    "    train_X = split_train.iloc[:, 3:].values\n",
    "    train_y = split_train.iloc[:, 2].values\n",
    "    \n",
    "    test_X = split_test.iloc[:, 3:].values\n",
    "    test_y = split_test.iloc[:, 2].values\n",
    "    \n",
    "    data_lst.append([[train_X,train_y],[test_X,test_y]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78bf5fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.71094965, 0.6071414 , 0.66616837, 0.54308243, 0.59673554,\n",
       "         0.51014486, 0.64550388, 0.5516656 , 0.5172243 , 0.52235017],\n",
       "        [0.7525998 , 0.62754802, 0.71992755, 0.67317934, 0.70290128,\n",
       "         0.57679997, 0.64177104, 0.5844612 , 0.6817547 , 0.60331811],\n",
       "        [0.57094409, 0.50026239, 0.48965384, 0.55105441, 0.59281946,\n",
       "         0.59672913, 0.49102359, 0.5802805 , 0.50105927, 0.55973703],\n",
       "        [0.55456018, 0.48502469, 0.44348991, 0.5748706 , 0.6701807 ,\n",
       "         0.64780626, 0.60076257, 0.57699046, 0.3418852 , 0.57218686],\n",
       "        [0.5563761 , 0.41146176, 0.55495914, 0.45852382, 0.47891037,\n",
       "         0.65847702, 0.46100798, 0.45092439, 0.55412222, 0.55897064],\n",
       "        [0.59687853, 0.69920276, 0.56772984, 0.66228864, 0.61587568,\n",
       "         0.66004142, 0.67895774, 0.58710225, 0.60429313, 0.55923498],\n",
       "        [0.51837613, 0.58830442, 0.54578292, 0.59432137, 0.51511336,\n",
       "         0.62127787, 0.56080146, 0.6181794 , 0.58891435, 0.51755801],\n",
       "        [0.61290346, 0.63625739, 0.59188835, 0.68669006, 0.57492419,\n",
       "         0.57162479, 0.58660579, 0.78235469, 0.4479915 , 0.59866736],\n",
       "        [0.52546974, 0.51805635, 0.55746679, 0.4998266 , 0.62282152,\n",
       "         0.67960027, 0.57338836, 0.65810045, 0.50981001, 0.63066837],\n",
       "        [0.49090659, 0.58684766, 0.54765044, 0.40568579, 0.49592731,\n",
       "         0.58904142, 0.6008613 , 0.5523441 , 0.48170498, 0.39291701],\n",
       "        [0.61785561, 0.40482863, 0.54562355, 0.50210086, 0.5299177 ,\n",
       "         0.5276714 , 0.59023796, 0.51777325, 0.52141449, 0.50399309],\n",
       "        [0.51411268, 0.51504341, 0.44978563, 0.33969102, 0.58992511,\n",
       "         0.63928365, 0.49853309, 0.50460848, 0.6016086 , 0.62032235],\n",
       "        [0.62197137, 0.7169422 , 0.59986698, 0.48562072, 0.58469369,\n",
       "         0.6762979 , 0.51132478, 0.61336371, 0.47804595, 0.49681278],\n",
       "        [0.35906776, 0.56785466, 0.51305794, 0.46714077, 0.42222965,\n",
       "         0.62604878, 0.5803121 , 0.50141151, 0.54008095, 0.46251446],\n",
       "        [0.58668809, 0.58840614, 0.80718252, 0.60557337, 0.55490057,\n",
       "         0.66426259, 0.62731121, 0.53597743, 0.66076033, 0.70082247],\n",
       "        [0.66591479, 0.47777754, 0.52542183, 0.47319164, 0.58997719,\n",
       "         0.65613008, 0.67711472, 0.57143971, 0.43046337, 0.49902321],\n",
       "        [0.63842389, 0.45206989, 0.56825007, 0.50256423, 0.53975976,\n",
       "         0.46348683, 0.60006808, 0.50208431, 0.39549315, 0.57249189],\n",
       "        [0.44473435, 0.3852048 , 0.55185827, 0.58704102, 0.6269487 ,\n",
       "         0.51353238, 0.64467937, 0.46082679, 0.58070932, 0.53528632],\n",
       "        [0.49823769, 0.66174884, 0.46014256, 0.66133306, 0.6940165 ,\n",
       "         0.74257073, 0.59600856, 0.78551077, 0.57533452, 0.53966791],\n",
       "        [0.61188207, 0.48189143, 0.52213276, 0.61360102, 0.60037858,\n",
       "         0.65321954, 0.59486936, 0.63564403, 0.52014887, 0.57075165],\n",
       "        [0.55155869, 0.47417467, 0.4445449 , 0.20358946, 0.57699451,\n",
       "         0.50611737, 0.37354832, 0.46609174, 0.37953413, 0.46529828],\n",
       "        [0.65617208, 0.47113575, 0.63293532, 0.71804724, 0.59532561,\n",
       "         0.65012576, 0.55939989, 0.6621931 , 0.57285362, 0.45299201],\n",
       "        [0.46908371, 0.48530471, 0.66490402, 0.54804183, 0.5504991 ,\n",
       "         0.60374194, 0.62476606, 0.5603764 , 0.54498253, 0.59317147],\n",
       "        [0.52585228, 0.58110588, 0.61988995, 0.57787398, 0.64265711,\n",
       "         0.61128289, 0.59176303, 0.61038757, 0.57188719, 0.48720211],\n",
       "        [0.55461176, 0.53521528, 0.57588853, 0.43908248, 0.54428458,\n",
       "         0.67376187, 0.52798163, 0.67632847, 0.73453447, 0.79702279],\n",
       "        [0.51051317, 0.55854303, 0.56729957, 0.48902402, 0.6926993 ,\n",
       "         0.64420667, 0.52650343, 0.67978897, 0.4726471 , 0.66776787],\n",
       "        [0.50930926, 0.42461944, 0.66548136, 0.46080129, 0.59992767,\n",
       "         0.59634487, 0.6479192 , 0.53273427, 0.59652775, 0.47050194],\n",
       "        [0.54170382, 0.5539854 , 0.57772672, 0.50263147, 0.53965367,\n",
       "         0.56273948, 0.58409397, 0.65632533, 0.52955073, 0.52192619],\n",
       "        [0.61908867, 0.65048057, 0.68428088, 0.54269681, 0.63919861,\n",
       "         0.57767885, 0.58134739, 0.68680071, 0.46827036, 0.53123139],\n",
       "        [0.53209892, 0.55075912, 0.50531356, 0.50752849, 0.50552402,\n",
       "         0.75642017, 0.61644501, 0.45748211, 0.47279303, 0.48508877],\n",
       "        [0.50750566, 0.43252258, 0.53767785, 0.57054245, 0.4283881 ,\n",
       "         0.56119375, 0.59488518, 0.50985042, 0.5176467 , 0.48535856],\n",
       "        [0.48868914, 0.48433435, 0.55227074, 0.44100694, 0.509416  ,\n",
       "         0.6657579 , 0.47886612, 0.4765284 , 0.46797819, 0.59241919],\n",
       "        [0.52496071, 0.63161683, 0.38294682, 0.41097812, 0.50531514,\n",
       "         0.61139975, 0.60545192, 0.62581762, 0.44513847, 0.53494919],\n",
       "        [0.50737345, 0.4207519 , 0.58596308, 0.39739003, 0.59302384,\n",
       "         0.48029621, 0.51957217, 0.51954635, 0.43198343, 0.45299904],\n",
       "        [0.45954407, 0.50012465, 0.63074076, 0.49352145, 0.56720405,\n",
       "         0.62784774, 0.55146821, 0.61207006, 0.56388065, 0.46675563],\n",
       "        [0.38550029, 0.53584205, 0.65163732, 0.55602554, 0.45107446,\n",
       "         0.60284059, 0.57834872, 0.62624744, 0.51169617, 0.53148522],\n",
       "        [0.60502205, 0.49409036, 0.59387208, 0.6618818 , 0.51774367,\n",
       "         0.7610559 , 0.58129261, 0.60431969, 0.48453801, 0.61927249],\n",
       "        [0.55181637, 0.48167068, 0.57821857, 0.51163919, 0.55330382,\n",
       "         0.61506998, 0.62837304, 0.50880983, 0.45521203, 0.57327599],\n",
       "        [0.54080111, 0.56057677, 0.50370036, 0.29105126, 0.65633247,\n",
       "         0.54568324, 0.60362413, 0.61216851, 0.46065236, 0.54502804],\n",
       "        [0.40866565, 0.42166178, 0.41047383, 0.31559725, 0.48494727,\n",
       "         0.55729298, 0.5756148 , 0.4297639 , 0.4612432 , 0.42476692],\n",
       "        [0.63302231, 0.51952959, 0.64841602, 0.49297609, 0.64570197,\n",
       "         0.68501744, 0.53088094, 0.45879036, 0.55879725, 0.56481818],\n",
       "        [0.64130342, 0.43743789, 0.7229485 , 0.60015614, 0.66583936,\n",
       "         0.74201642, 0.60573174, 0.66214109, 0.55354826, 0.64133044],\n",
       "        [0.5274835 , 0.61896372, 0.64896265, 0.50759967, 0.67161368,\n",
       "         0.58645759, 0.56608138, 0.64287308, 0.68061604, 0.75186078],\n",
       "        [0.50183359, 0.5170058 , 0.45945463, 0.30098084, 0.59352715,\n",
       "         0.67743174, 0.59152975, 0.42183765, 0.37753436, 0.48943496],\n",
       "        [0.54741794, 0.54066532, 0.61156192, 0.55799718, 0.43069099,\n",
       "         0.58093126, 0.5913824 , 0.71118722, 0.48682257, 0.52378493],\n",
       "        [0.63088305, 0.39377607, 0.56666876, 0.49840855, 0.50966942,\n",
       "         0.63730716, 0.5873852 , 0.54638032, 0.4579361 , 0.49416771],\n",
       "        [0.6357681 , 0.61717997, 0.66533743, 0.62288494, 0.67313767,\n",
       "         0.63137752, 0.64718319, 0.62553536, 0.46852232, 0.62503296],\n",
       "        [0.68931902, 0.5794545 , 0.63176903, 0.54260372, 0.53288117,\n",
       "         0.60618182, 0.52214792, 0.57561934, 0.43572776, 0.70003408],\n",
       "        [0.47641975, 0.54974259, 0.45913963, 0.57007805, 0.60593363,\n",
       "         0.61639573, 0.62507654, 0.51004704, 0.60402005, 0.43106669],\n",
       "        [0.50817862, 0.63343694, 0.53954605, 0.5376239 , 0.64856187,\n",
       "         0.56996852, 0.71328688, 0.52490003, 0.50007168, 0.61819813],\n",
       "        [0.50189532, 0.55569894, 0.70464872, 0.53863754, 0.55292448,\n",
       "         0.60460261, 0.75535633, 0.63173445, 0.4945058 , 0.55032348],\n",
       "        [0.47328532, 0.49421776, 0.64601533, 0.42153375, 0.61123971,\n",
       "         0.59224804, 0.71444298, 0.51328197, 0.58083466, 0.5204308 ],\n",
       "        [0.51162937, 0.43817119, 0.51654202, 0.40258952, 0.53518195,\n",
       "         0.56635572, 0.61840972, 0.60113055, 0.55283894, 0.43342396],\n",
       "        [0.60957652, 0.67602707, 0.49521826, 0.61804295, 0.62192277,\n",
       "         0.60507332, 0.65383783, 0.67211716, 0.59443476, 0.73976685],\n",
       "        [0.52029901, 0.52773961, 0.49294909, 0.3213517 , 0.50098481,\n",
       "         0.52874867, 0.43331205, 0.4083106 , 0.41506219, 0.31108477],\n",
       "        [0.52987075, 0.52104478, 0.57660916, 0.55734186, 0.49445897,\n",
       "         0.57765236, 0.66802676, 0.52075543, 0.52191011, 0.50376101],\n",
       "        [0.57665089, 0.59209166, 0.52392793, 0.58669055, 0.5440507 ,\n",
       "         0.67279755, 0.638695  , 0.66514836, 0.48360808, 0.62554342],\n",
       "        [0.48179046, 0.56081569, 0.64457513, 0.50536164, 0.51915745,\n",
       "         0.68900089, 0.67492973, 0.60107405, 0.63345014, 0.48914719],\n",
       "        [0.570247  , 0.47505528, 0.47639979, 0.4392118 , 0.52467056,\n",
       "         0.56700908, 0.52089208, 0.4906108 , 0.45241341, 0.53901439],\n",
       "        [0.6066694 , 0.58996981, 0.53256583, 0.45078165, 0.586091  ,\n",
       "         0.57007611, 0.54890122, 0.44469102, 0.54873608, 0.52840909],\n",
       "        [0.77530025, 0.64396646, 0.57802482, 0.58433734, 0.63466081,\n",
       "         0.51330069, 0.58735521, 0.45485418, 0.48597092, 0.51258522],\n",
       "        [0.48598833, 0.57801791, 0.56565735, 0.59653007, 0.56252217,\n",
       "         0.57993892, 0.49651457, 0.67303231, 0.44219972, 0.52111563],\n",
       "        [0.56782193, 0.54666384, 0.44172543, 0.53490814, 0.6011772 ,\n",
       "         0.57756774, 0.69762798, 0.63914978, 0.68244258, 0.48579851],\n",
       "        [0.51293557, 0.42266143, 0.58408173, 0.5553093 , 0.52917176,\n",
       "         0.6281719 , 0.53348472, 0.52658889, 0.59210012, 0.54947125],\n",
       "        [0.50387527, 0.59158059, 0.51214639, 0.57565035, 0.55247376,\n",
       "         0.50400226, 0.71711925, 0.63702581, 0.48636806, 0.59611082],\n",
       "        [0.71956351, 0.45373177, 0.6862482 , 0.50028134, 0.54077912,\n",
       "         0.50365348, 0.58101668, 0.54335597, 0.3818939 , 0.58925257],\n",
       "        [0.56240428, 0.55548566, 0.62665265, 0.47548626, 0.62598143,\n",
       "         0.60219085, 0.53297046, 0.57815138, 0.56561928, 0.57768385],\n",
       "        [0.41614618, 0.49492219, 0.43309207, 0.28505997, 0.52784189,\n",
       "         0.45992097, 0.54660617, 0.28127695, 0.49854934, 0.40119833],\n",
       "        [0.56687539, 0.70860287, 0.50684323, 0.44645873, 0.50400653,\n",
       "         0.49462448, 0.62711933, 0.57144115, 0.48322378, 0.62985061],\n",
       "        [0.53226919, 0.61041569, 0.66173434, 0.60129927, 0.64191363,\n",
       "         0.64908756, 0.7947431 , 0.5587517 , 0.5824686 , 0.63234423],\n",
       "        [0.57179376, 0.58275654, 0.60488287, 0.6108576 , 0.55442816,\n",
       "         0.57722602, 0.56487684, 0.5337262 , 0.48110907, 0.67475118],\n",
       "        [0.58255294, 0.61970383, 0.66964536, 0.53632735, 0.55173988,\n",
       "         0.64971416, 0.61852364, 0.57850455, 0.5279137 , 0.50190639],\n",
       "        [0.50314921, 0.56761515, 0.58426139, 0.53796023, 0.55935825,\n",
       "         0.59007339, 0.62979517, 0.51341109, 0.5471252 , 0.54766542],\n",
       "        [0.61242665, 0.42991623, 0.55804425, 0.5648625 , 0.45987551,\n",
       "         0.65861104, 0.69808   , 0.48176343, 0.4882969 , 0.60711848],\n",
       "        [0.64059981, 0.60459802, 0.59976764, 0.57063574, 0.61006722,\n",
       "         0.75372961, 0.67000069, 0.61299129, 0.61722107, 0.65673999],\n",
       "        [0.69927356, 0.49408811, 0.56241317, 0.54213934, 0.56158797,\n",
       "         0.45099239, 0.65314362, 0.4638491 , 0.46339956, 0.44436725],\n",
       "        [0.58476253, 0.60210783, 0.46568507, 0.602878  , 0.69330366,\n",
       "         0.7071876 , 0.68598085, 0.6835573 , 0.69764833, 0.68885108],\n",
       "        [0.651588  , 0.58494974, 0.67187871, 0.56177022, 0.66277516,\n",
       "         0.71137551, 0.62961466, 0.57801295, 0.57367275, 0.64406957],\n",
       "        [0.55529867, 0.5976384 , 0.55330876, 0.52302501, 0.52543144,\n",
       "         0.66751686, 0.50239743, 0.60175636, 0.4943718 , 0.63772911],\n",
       "        [0.48190458, 0.49560705, 0.47945469, 0.45301243, 0.56662352,\n",
       "         0.59648622, 0.56033827, 0.57175117, 0.51447755, 0.43036094]]),\n",
       " array([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0], dtype=int64)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lst[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa77a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model, referenced tutorial\n",
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, input_size = None):\n",
    "        super().__init__()  \n",
    "        layers = []\n",
    "        #input\n",
    "        layers.append(nn.Linear(input_size, 8))\n",
    "        layers.append(nn.ReLU())\n",
    "        #hidden\n",
    "        layers.append(nn.Linear(8,6))\n",
    "        layers.append(nn.ReLU())\n",
    "        #output\n",
    "        layers.append(nn.Linear(6, 2))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self(X)\n",
    "            return int(pred.argmax().detach())     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba50bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNClassifier(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=6, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=6, out_features=2, bias=True)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_len = 10\n",
    "model = NNClassifier(input_len)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78fb7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referenced from tutorial\n",
    "def train_model(model, train_dl, optimizer, loss_f, nr_epochs, print_loss_every=10):\n",
    "    for t in range(nr_epochs):\n",
    "        model.train()\n",
    "        nr_batches = len(train_dl)\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        pred_lst = []\n",
    "        target_lst = []\n",
    "        \n",
    "        for _, (X, y) in enumerate(train_dl):\n",
    "            pred = model(X)\n",
    "            loss = loss_f(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred_lst.append(pred.detach().numpy())\n",
    "            target_lst.append(y.detach().numpy())\n",
    "        \n",
    "        pred_lst = np.concatenate(pred_lst, axis=0)\n",
    "        target_lst =  np.concatenate(target_lst, axis=0)\n",
    "\n",
    "        \n",
    "        if t % print_loss_every == 0:\n",
    "            print(f\"Epoch {t} loss {total_loss / nr_batches}, accuracy {metrics.accuracy_score(target_lst, pred_lst.argmax(1))}\")\n",
    "            \n",
    "    print(\"Training complete\")\n",
    "    \n",
    "def eval_model(model, dl):\n",
    "    model.eval()\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    for _, (X, y) in enumerate(dl):\n",
    "        pred = model(X)\n",
    "        targets += list(y.detach().cpu().numpy())\n",
    "        predictions += list(pred.argmax(1).detach().cpu().numpy())\n",
    "        \n",
    "    accuracy = metrics.accuracy_score(targets, predictions)\n",
    "    precision = metrics.precision_score(targets,predictions)\n",
    "    recall = metrics.recall_score(targets,predictions)\n",
    "    fpr, tpr, _ = metrics.roc_curve(targets, predictions, pos_label=1)\n",
    "    auc = metrics.auc(fpr,tpr)\n",
    "    print('Accuracy: '+ str(accuracy))\n",
    "    print('Precision: '+ str(precision))\n",
    "    print('Recall: ' + str(recall))\n",
    "    print('AUC: ' + str(auc))\n",
    "    \n",
    "    return [accuracy,precision,recall,auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7d8be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class df2Dataset(Dataset):\n",
    "    def __init__(self, df_lst, y_dtype=torch.int64):\n",
    "        self.X = torch.tensor(df_lst[0],dtype=torch.float32)\n",
    "        self.y = torch.tensor(df_lst[1],dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37a11003",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: 1\n",
      "Epoch 0 loss 0.6959626384079456, accuracy 0.5\n",
      "Epoch 10 loss 0.693686268478632, accuracy 0.4625\n",
      "Epoch 20 loss 0.6928190261125564, accuracy 0.5375\n",
      "Epoch 30 loss 0.6898744039237499, accuracy 0.6\n",
      "Epoch 40 loss 0.6777487851679325, accuracy 0.625\n",
      "Epoch 50 loss 0.6374081518501044, accuracy 0.675\n",
      "Epoch 60 loss 0.5984674904495477, accuracy 0.675\n",
      "Epoch 70 loss 0.5775482771918178, accuracy 0.6625\n",
      "Epoch 80 loss 0.5655128189362586, accuracy 0.6625\n",
      "Epoch 90 loss 0.5588103746064007, accuracy 0.6625\n",
      "Training complete\n",
      "Accuracy: 0.55\n",
      "Precision: 0.6\n",
      "Recall: 0.3\n",
      "AUC: 0.55\n",
      "Current fold: 2\n",
      "Epoch 0 loss 0.7166630454361439, accuracy 0.4875\n",
      "Epoch 10 loss 0.6908059202134609, accuracy 0.5125\n",
      "Epoch 20 loss 0.6875861465930939, accuracy 0.575\n",
      "Epoch 30 loss 0.681240015476942, accuracy 0.6625\n",
      "Epoch 40 loss 0.6679650418460369, accuracy 0.7\n",
      "Epoch 50 loss 0.6446383655071258, accuracy 0.6625\n",
      "Epoch 60 loss 0.6129213776439428, accuracy 0.6875\n",
      "Epoch 70 loss 0.5813908971846103, accuracy 0.7125\n",
      "Epoch 80 loss 0.558146721124649, accuracy 0.725\n",
      "Epoch 90 loss 0.5446958223357796, accuracy 0.7375\n",
      "Training complete\n",
      "Accuracy: 0.6\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.5454545454545454\n",
      "AUC: 0.6060606060606062\n",
      "Current fold: 3\n",
      "Epoch 0 loss 0.7023534566164017, accuracy 0.5\n",
      "Epoch 10 loss 0.6907575480639935, accuracy 0.55\n",
      "Epoch 20 loss 0.6843234196305275, accuracy 0.575\n",
      "Epoch 30 loss 0.669782280176878, accuracy 0.5375\n",
      "Epoch 40 loss 0.6420777395367623, accuracy 0.6\n",
      "Epoch 50 loss 0.616417202167213, accuracy 0.6375\n",
      "Epoch 60 loss 0.5996925293467938, accuracy 0.6875\n",
      "Epoch 70 loss 0.5894471582025289, accuracy 0.7\n",
      "Epoch 80 loss 0.5839170856866985, accuracy 0.7\n",
      "Epoch 90 loss 0.5763989087194205, accuracy 0.7\n",
      "Training complete\n",
      "Accuracy: 0.8\n",
      "Precision: 1.0\n",
      "Recall: 0.6363636363636364\n",
      "AUC: 0.8181818181818181\n",
      "Current fold: 4\n",
      "Epoch 0 loss 0.7095378398895263, accuracy 0.475\n",
      "Epoch 10 loss 0.6924258232116699, accuracy 0.525\n",
      "Epoch 20 loss 0.6918754331767559, accuracy 0.525\n",
      "Epoch 30 loss 0.6911086231470108, accuracy 0.525\n",
      "Epoch 40 loss 0.6889823287725448, accuracy 0.525\n",
      "Epoch 50 loss 0.6831035286188125, accuracy 0.525\n",
      "Epoch 60 loss 0.6695374179631471, accuracy 0.5875\n",
      "Epoch 70 loss 0.6495661213994026, accuracy 0.6375\n",
      "Epoch 80 loss 0.6307441752403975, accuracy 0.6625\n",
      "Epoch 90 loss 0.616804749891162, accuracy 0.675\n",
      "Training complete\n",
      "Accuracy: 0.6\n",
      "Precision: 0.5\n",
      "Recall: 0.875\n",
      "AUC: 0.6458333333333333\n",
      "Current fold: 5\n",
      "Epoch 0 loss 0.6937015555799008, accuracy 0.4625\n",
      "Epoch 10 loss 0.6897347539663314, accuracy 0.55\n",
      "Epoch 20 loss 0.672818337380886, accuracy 0.5875\n",
      "Epoch 30 loss 0.6389885999262332, accuracy 0.6375\n",
      "Epoch 40 loss 0.6119126254692674, accuracy 0.65\n",
      "Epoch 50 loss 0.5977707125246525, accuracy 0.65\n",
      "Epoch 60 loss 0.5896143560297787, accuracy 0.675\n",
      "Epoch 70 loss 0.5843872875906527, accuracy 0.675\n",
      "Epoch 80 loss 0.5806582557037473, accuracy 0.6875\n",
      "Epoch 90 loss 0.5782159908674658, accuracy 0.6875\n",
      "Training complete\n",
      "Accuracy: 0.55\n",
      "Precision: 0.5714285714285714\n",
      "Recall: 0.4\n",
      "AUC: 0.5499999999999999\n"
     ]
    }
   ],
   "source": [
    "#Train model and evalute\n",
    "score_lst = []\n",
    "fold_cnt = 1\n",
    "no_epoch = 100\n",
    "\n",
    "\n",
    "for i in data_lst:\n",
    "    train_dl = DataLoader(df2Dataset(i[0])) \n",
    "    test_dl = DataLoader(df2Dataset(i[1]))\n",
    "    print(\"Current fold: \"+ str(fold_cnt))\n",
    "    fold_cnt+=1\n",
    "    \n",
    "    model = NNClassifier(10) \n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_model(model, train_dl, optimizer, loss_function,no_epoch, )\n",
    "    score_lst.append(eval_model(model,test_dl))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19310977",
   "metadata": {},
   "source": [
    "### Log Regression and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3fd9ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lst[0][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b5d1a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: 1\n",
      "log test accuracy:  0.8\n",
      "svm test accuracy:  0.7\n",
      "Current fold: 2\n",
      "log test accuracy:  0.55\n",
      "svm test accuracy:  0.55\n",
      "Current fold: 3\n",
      "log test accuracy:  0.85\n",
      "svm test accuracy:  0.55\n",
      "Current fold: 4\n",
      "log test accuracy:  0.55\n",
      "svm test accuracy:  0.4\n",
      "Current fold: 5\n",
      "log test accuracy:  0.65\n",
      "svm test accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "svm_score_lst = []\n",
    "log_score_lst = []\n",
    "\n",
    "fold_cnt = 1\n",
    "for i in data_lst:\n",
    "    print(\"Current fold: \"+ str(fold_cnt))\n",
    "    fold_cnt+=1\n",
    "    \n",
    "    X_train = i[0][0]\n",
    "    Y_train = i[0][1]\n",
    "    X_test = i[1][0]\n",
    "    Y_test = i[1][1]\n",
    "    \n",
    "    \n",
    "    log_model = LogisticRegression(random_state=0, max_iter=200)\n",
    "    log_model.fit(X_train, Y_train)\n",
    "\n",
    "    logtrain_pred = log_model.predict(X_train)\n",
    "    logtest_pred = log_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    accuracy = metrics.accuracy_score(Y_test,logtest_pred)\n",
    "    precision = metrics.precision_score(Y_test,logtest_pred)\n",
    "    recall = metrics.recall_score(Y_test,logtest_pred)\n",
    "    fpr, tpr, _ = metrics.roc_curve(Y_test,logtest_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr,tpr)\n",
    "    print(\"log test accuracy: \", accuracy)\n",
    "\n",
    "    log_score_lst.append([accuracy,precision,recall,auc])\n",
    "    \n",
    "    svm = SVC(gamma='auto', kernel='linear')\n",
    "    svm.fit(X_train, Y_train)\n",
    "    svmtrain_pred = svm.predict(X_train)\n",
    "    svmtest_pred = svm.predict(X_test)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(Y_test,svmtest_pred)\n",
    "    precision = metrics.precision_score(Y_test,svmtest_pred)\n",
    "    recall = metrics.recall_score(Y_test,svmtest_pred)\n",
    "    fpr, tpr, _ = metrics.roc_curve(Y_test,svmtest_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr,tpr)\n",
    "    print(\"svm test accuracy: \", accuracy)\n",
    "    \n",
    "    log_score_lst.append([accuracy,precision,recall,auc])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad2c12",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_4 = ''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
