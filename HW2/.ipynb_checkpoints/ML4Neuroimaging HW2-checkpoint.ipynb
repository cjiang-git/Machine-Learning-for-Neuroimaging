{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f97be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b1a96",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098ec94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>Frontal_Sup</th>\n",
       "      <th>Frontal_Inf</th>\n",
       "      <th>Cingulum_Ant</th>\n",
       "      <th>Cingulum_Post</th>\n",
       "      <th>Parietal_Sup</th>\n",
       "      <th>Parietal_Inf</th>\n",
       "      <th>Occipital_Sup</th>\n",
       "      <th>Occipital_Inf</th>\n",
       "      <th>Temporal_Sup</th>\n",
       "      <th>Temporal_Inf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.368493</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.541704</td>\n",
       "      <td>0.553985</td>\n",
       "      <td>0.577727</td>\n",
       "      <td>0.502631</td>\n",
       "      <td>0.539654</td>\n",
       "      <td>0.562739</td>\n",
       "      <td>0.584094</td>\n",
       "      <td>0.656325</td>\n",
       "      <td>0.529551</td>\n",
       "      <td>0.521926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.654435</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.665915</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.525422</td>\n",
       "      <td>0.473192</td>\n",
       "      <td>0.589977</td>\n",
       "      <td>0.656130</td>\n",
       "      <td>0.677115</td>\n",
       "      <td>0.571440</td>\n",
       "      <td>0.430463</td>\n",
       "      <td>0.499023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.998386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.385500</td>\n",
       "      <td>0.535842</td>\n",
       "      <td>0.651637</td>\n",
       "      <td>0.556026</td>\n",
       "      <td>0.451074</td>\n",
       "      <td>0.602841</td>\n",
       "      <td>0.578349</td>\n",
       "      <td>0.626247</td>\n",
       "      <td>0.511696</td>\n",
       "      <td>0.531485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.640988</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.601210</td>\n",
       "      <td>0.500756</td>\n",
       "      <td>0.614601</td>\n",
       "      <td>0.581634</td>\n",
       "      <td>0.489078</td>\n",
       "      <td>0.615674</td>\n",
       "      <td>0.579836</td>\n",
       "      <td>0.548231</td>\n",
       "      <td>0.499937</td>\n",
       "      <td>0.612477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.878145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.529871</td>\n",
       "      <td>0.521045</td>\n",
       "      <td>0.576609</td>\n",
       "      <td>0.557342</td>\n",
       "      <td>0.494459</td>\n",
       "      <td>0.577652</td>\n",
       "      <td>0.668027</td>\n",
       "      <td>0.520755</td>\n",
       "      <td>0.521910</td>\n",
       "      <td>0.503761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>45.966536</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.555299</td>\n",
       "      <td>0.597638</td>\n",
       "      <td>0.553309</td>\n",
       "      <td>0.523025</td>\n",
       "      <td>0.525431</td>\n",
       "      <td>0.667517</td>\n",
       "      <td>0.502397</td>\n",
       "      <td>0.601756</td>\n",
       "      <td>0.494372</td>\n",
       "      <td>0.637729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>48.160286</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.524961</td>\n",
       "      <td>0.631617</td>\n",
       "      <td>0.382947</td>\n",
       "      <td>0.410978</td>\n",
       "      <td>0.505315</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.605452</td>\n",
       "      <td>0.625818</td>\n",
       "      <td>0.445138</td>\n",
       "      <td>0.534949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>32.339052</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490657</td>\n",
       "      <td>0.652892</td>\n",
       "      <td>0.654747</td>\n",
       "      <td>0.656929</td>\n",
       "      <td>0.521088</td>\n",
       "      <td>0.702478</td>\n",
       "      <td>0.640216</td>\n",
       "      <td>0.537376</td>\n",
       "      <td>0.642808</td>\n",
       "      <td>0.598981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>33.315738</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.584763</td>\n",
       "      <td>0.602108</td>\n",
       "      <td>0.465685</td>\n",
       "      <td>0.602878</td>\n",
       "      <td>0.693304</td>\n",
       "      <td>0.707188</td>\n",
       "      <td>0.685981</td>\n",
       "      <td>0.683557</td>\n",
       "      <td>0.697648</td>\n",
       "      <td>0.688851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>32.630768</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.498238</td>\n",
       "      <td>0.661749</td>\n",
       "      <td>0.460143</td>\n",
       "      <td>0.661333</td>\n",
       "      <td>0.694017</td>\n",
       "      <td>0.742571</td>\n",
       "      <td>0.596009</td>\n",
       "      <td>0.785511</td>\n",
       "      <td>0.575335</td>\n",
       "      <td>0.539668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  sex  diagnosis  Frontal_Sup  Frontal_Inf  Cingulum_Ant  \\\n",
       "0   43.368493    0          0     0.541704     0.553985      0.577727   \n",
       "1   36.654435    0          0     0.665915     0.477778      0.525422   \n",
       "2   37.998386    0          0     0.385500     0.535842      0.651637   \n",
       "3   36.640988    0          0     0.601210     0.500756      0.614601   \n",
       "4   42.878145    0          0     0.529871     0.521045      0.576609   \n",
       "..        ...  ...        ...          ...          ...           ...   \n",
       "95  45.966536    1          1     0.555299     0.597638      0.553309   \n",
       "96  48.160286    1          1     0.524961     0.631617      0.382947   \n",
       "97  32.339052    1          1     0.490657     0.652892      0.654747   \n",
       "98  33.315738    1          1     0.584763     0.602108      0.465685   \n",
       "99  32.630768    1          1     0.498238     0.661749      0.460143   \n",
       "\n",
       "    Cingulum_Post  Parietal_Sup  Parietal_Inf  Occipital_Sup  Occipital_Inf  \\\n",
       "0        0.502631      0.539654      0.562739       0.584094       0.656325   \n",
       "1        0.473192      0.589977      0.656130       0.677115       0.571440   \n",
       "2        0.556026      0.451074      0.602841       0.578349       0.626247   \n",
       "3        0.581634      0.489078      0.615674       0.579836       0.548231   \n",
       "4        0.557342      0.494459      0.577652       0.668027       0.520755   \n",
       "..            ...           ...           ...            ...            ...   \n",
       "95       0.523025      0.525431      0.667517       0.502397       0.601756   \n",
       "96       0.410978      0.505315      0.611400       0.605452       0.625818   \n",
       "97       0.656929      0.521088      0.702478       0.640216       0.537376   \n",
       "98       0.602878      0.693304      0.707188       0.685981       0.683557   \n",
       "99       0.661333      0.694017      0.742571       0.596009       0.785511   \n",
       "\n",
       "    Temporal_Sup  Temporal_Inf  \n",
       "0       0.529551      0.521926  \n",
       "1       0.430463      0.499023  \n",
       "2       0.511696      0.531485  \n",
       "3       0.499937      0.612477  \n",
       "4       0.521910      0.503761  \n",
       "..           ...           ...  \n",
       "95      0.494372      0.637729  \n",
       "96      0.445138      0.534949  \n",
       "97      0.642808      0.598981  \n",
       "98      0.697648      0.688851  \n",
       "99      0.575335      0.539668  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data_assignment_1.csv'\n",
    "\n",
    "#Data import\n",
    "df3 = pd.read_csv(data_path)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, save_path=\"models\"):\n",
    "        super().__init__()\n",
    "        self.name = \"NN Model\"\n",
    "        self.save_path = save_path\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        # Define the architecture of the model\n",
    "        layers = []\n",
    "        nr_neurons = [input_size] + hidden_layer_sizes\n",
    "        for ix in range(len(nr_neurons)-1):\n",
    "            # A linear, fully-connected layer\n",
    "            layers.append(nn.Linear(nr_neurons[ix], nr_neurons[ix+1]))\n",
    "            # A ReLU activation function\n",
    "            layers.append(nn.ReLU())\n",
    "        # Final layer, in this case for binary (nr_classes==2) classification\n",
    "        layers.append(nn.Linear(nr_neurons[-1], nr_classes))\n",
    "        # Finally, we place them one after the other\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # We'll sometimes need to flatten the inputs (not necessary in this case)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # The softmax function ensures we have one output per class, and these add up to 1\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Make a prediction based on a given input'''\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self(X)\n",
    "            return int(pred.argmax().detach())     \n",
    "        \n",
    "    def save(self, state_name='last', verbose=False):\n",
    "        '''Saves a model state in the defined path, with the model name'''\n",
    "        model_state_name = self.name+'_'+state_name+'.pth'\n",
    "        torch.save(self.state_dict(), os.path.join(self.save_path, model_state_name))\n",
    "        if verbose:\n",
    "            print(\"Saved PyTorch model state {} in {}\".format(model_state_name, self.save_path))\n",
    "            \n",
    "    def restore(self, state_name):\n",
    "        '''Restores a model state for the given state name'''\n",
    "        model_state_name = self.name+'_'+state_name+'.pth'\n",
    "        self.load_state_dict(torch.load(os.path.join(self.save_path, model_state_name)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
